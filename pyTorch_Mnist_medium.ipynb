{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch-Mnist-medium.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/plouismarie/pytorchMnist-medium/blob/master/pyTorch_Mnist_medium.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YmfVeK1smGGM",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[Quick Introduction to computer vision with MNIST and Pytorch.](#scrollTo=RmXc2y0rmSoA)\n",
        "\n",
        ">>>[Computer Vision](#scrollTo=RmXc2y0rmSoA)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RmXc2y0rmSoA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quick Introduction to computer vision with MNIST and Pytorch.\n",
        "\n",
        "\n",
        "Who should read ? :\n",
        "This tutorial is for beginner in AI, looking for practical code I am happy to share, or people who want to refresh their basic knowledge about AI and computer vision.\n",
        "\n",
        "Which Framework ? :\n",
        "Pytorch\n",
        "\n",
        "How long to read ? :\n",
        "~7 mn\n",
        "\n",
        "The Goal ? : We will see how to use AI to recognize an handwritten digit of yours, along to some common errors that are explained and solved.\n",
        "\n",
        "<u>Intro</u>: When you are a beginner in AI and computer vision, it is easy to get confused with all the terminologies.\n",
        "“you could create a successful net without understanding how it worked”\n",
        "This will remain high level and will not go to deep math theory, then my readers stay up to the end of the article ;) but coding section would be practical and complete.\n",
        "\n",
        "Let’s start with some popular terms, just a small selection to start with, and see why it is required to be clear on that.\n",
        "\n",
        "### Computer Vision\n",
        "\n",
        "We use a network of digital neurons to give machine the aptitude to recognize a shape, an object or a living entity from a picture, close to the human being efficiency. Back in the end of the 50's, one of an early machine learning classifier was actually named perceptron, for digital perception.\n",
        "\n",
        "![image.png](image.png)\n",
        "\n",
        "<i>image from Michael Steven's youtube video : What Is The Resolution Of The Eye?</i>"
      ]
    },
    {
      "metadata": {
        "id": "pYam93w7t7K1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "6e2abca2-1b4c-4d62-ff06-c85c5b07c9d2"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision torchsummary\n",
        "!pip install --no-cache-dir -I pillow\n",
        "print(accelerator)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x560ec000 @  0x7f548c5fb1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 26.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n",
            "cu80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v9MVAGEqrVMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3c31421-ebc7-4e0e-d4cf-63f5e0f5d789"
      },
      "cell_type": "code",
      "source": [
        "# import all the needed libraries\n",
        "\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from torch import optim\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import math,os\n",
        "\n",
        "from PIL import Image,ImageOps\n",
        "\n",
        "print('##Import : OK')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##Import : OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MJJl3qehvGi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5383113-057f-49cc-e6e0-0bdfa2fb8ab5"
      },
      "cell_type": "code",
      "source": [
        "# device variable will be used later to point to appropriate device directly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)#shows cuda if gpu is detected else shows cpu\n",
        "print(\"Pytorch: {}\".format(torch.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Pytorch: 0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OsDp_TwnyUmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() # python 3 allows quick syntax here instead of using super(Net,self)  \n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, #input\n",
        "            10, #output nb_filters\n",
        "            stride=1, #filter step\n",
        "            kernel_size=5) #filter size\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d() # randomly zero some input - default is 50%\n",
        "        self.fc1 = nn.Linear(320, 50) # fully connected layer\n",
        "        self.fc2 = nn.Linear(50, 10) # output is 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320) # reshape\n",
        "        x = F.relu(self.fc1(x)) # activation\n",
        "        x = F.dropout(x, training=self.training)# dropout prevents the model from placing too much trust in any of the input features\n",
        "        x = self.fc2(x) # reduce the output\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UgAwaNRRyaht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jx0p4Gsyyf3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "870d6d4d-acfd-4411-afec-552ab186c4a1"
      },
      "cell_type": "code",
      "source": [
        "def getnormalization_values(): \n",
        "    train_dataset=torchvision.datasets.MNIST(root='data/', train=True, transform=transforms.Compose([transforms.ToTensor()]), download=True)\n",
        "    mean=float(train_dataset.train_data.float().mean()/255)\n",
        "    std=float(train_dataset.train_data.float().std()/255)\n",
        "    train_sample=train_dataset[0][0]\n",
        "    return mean,std,train_sample\n",
        "# for dataset with color and natural image, like Cifar, formula would be different\n",
        "# https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/12\n",
        "\n",
        "# hyper parameters\n",
        "batch_size = 32\n",
        "\n",
        "mean,std,x00=getnormalization_values()\n",
        "\n",
        "#Define transformations\n",
        "transformations = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((mean,mean,mean),(std,std,std))\n",
        "])\n",
        "\n",
        "# get datasets, with the auto extracted normalization values\n",
        "train_dataset = torchvision.datasets.MNIST(root='data/', train=True, transform=transformations, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='data/', train=False, transform=transformations)\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "id=random.randint(0,len(train_loader))\n",
        "sample=train_dataset[id][0]\n",
        "#pic_height = 28 # let's not hardcode it and let the code guess it \n",
        "#pic_weight = 28 # by extracting actual value from the training set\n",
        "pic_height=tuple(sample.size())[1] #28 for mnist, 32 for cifar, 224 for imagenet\n",
        "pic_weight=tuple(sample.size())[2] #28 for mnist, 32 for cifar, 224 for imagenet\n",
        "\n",
        "print('dataset is ready')\n",
        "print('dataset dimension:',tuple(sample.size()))\n",
        "\n",
        "print('Number of elements:',list(train_dataset.train_data.size())[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "dataset is ready\n",
            "dataset dimension: (1, 28, 28)\n",
            "Number of elements: 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PO_kjjsryu3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "5a038c4e-cd30-494b-d287-c76425363b90"
      },
      "cell_type": "code",
      "source": [
        "# The learning rate function below dynamically adjusts itself\n",
        "\n",
        "def isNotImproving(acc_list,nbitems):\n",
        "    # https://stackoverflow.com/questions/30647330/calculate-if-trend-is-up-down-or-stable\n",
        "    acc_list=acc_list[-nbitems:]\n",
        "    A10=sum(range(nbitems+1))\n",
        "    B10=sum(acc_list)\n",
        "    C10=0\n",
        "    for i in range(nbitems): C10=C10 + (i+1) * acc_list[i]\n",
        "    D10=0\n",
        "    for i in range(nbitems): D10=D10 + (i+1) * (i+1)\n",
        "    b=(nbitems * C10 - A10 * B10) / (nbitems * D10 - A10 * A10)\n",
        "    #print('b:',b)\n",
        "    if b <= 0: return True\n",
        "    return False\n",
        "\n",
        "def adaptive_learning_rate(count,optimizer,lr,acc_list,patience=3,nbitems=5):\n",
        "    if len(acc_list) > nbitems:\n",
        "        if isNotImproving(acc_list,nbitems):\n",
        "            count+=1  \n",
        "            print('===non increasing trend detected-',count,'/',patience)\n",
        "    if count >= patience:\n",
        "        print('===patience is exceeded-steppingLR')\n",
        "        lr=lr/10\n",
        "        count=0\n",
        "        state_dict=optimizer.state_dict()\n",
        "        for param_group in state_dict['param_groups']:\n",
        "            param_group[\"lr\"]=lr\n",
        "        optimizer.load_state_dict(state_dict)\n",
        "    return count,optimizer,lr\n",
        "\n",
        "# train the model\n",
        "\n",
        "def train(model,train_loader,optimizer,learning_rate,num_epochs,criterion,device,pic_weight,pic_height):\n",
        "    train_loss_list,train_accuracy_list=[],[]\n",
        "    train_accuracy,total,count=0,0,0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i,(images,labels) in enumerate(train_loader):\n",
        "            images=images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            if torch.cuda.is_available(): model.cuda()\n",
        "            outputs=model(images)\n",
        "            #compute the loss based on the predictions and actual labels\n",
        "            train_loss=criterion(outputs,labels)\n",
        "            #Accuracy\n",
        "            _,prediction=torch.max(outputs.data,1)\n",
        "            total += labels.size(0)\n",
        "            train_accuracy += (prediction == labels).sum().item()\n",
        "            #Clear all accumulated gradients            \n",
        "            optimizer.zero_grad()\n",
        "            #Backpropagate the loss\n",
        "            train_loss.backward()\n",
        "            #adjust parameters according to the computed gradients\n",
        "            optimizer.step()\n",
        "            #display some stats\n",
        "            if (i+1) % 100 == 0:\n",
        "                train_accuracy_pct=100 * train_accuracy / total\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Accuracy [{}/{}] {:.2f}%, Loss: {:.4f}, lr:{}'.format(\n",
        "                  epoch+1, num_epochs,\n",
        "                  i+1, len(train_loader),\n",
        "                  train_accuracy,total,train_accuracy_pct,\n",
        "                  train_loss.item(),\n",
        "                  learning_rate))\n",
        "                train_loss_list.append(train_loss.item()) #then we could plot the loss\n",
        "                train_accuracy_list.append(train_accuracy_pct) #then we will plot the accuracy\n",
        "                count,optimizer,learning_rate=adaptive_learning_rate(count,optimizer,learning_rate,train_accuracy_list)\n",
        "    return train_loss_list,train_accuracy_list\n",
        "\n",
        "def display_summary(model,sample):\n",
        "    try:\n",
        "        summary(model,tuple(sample.size()))\n",
        "    except:\n",
        "        print(model) # display AI model in use\n",
        "        print('Total params: ',sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "model=Net().to(device)\n",
        "display_summary(model,sample)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a9cfa1522f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdisplay_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "T1Qg8frFy0w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Loss and optimizer\n",
        "learning_rate = 0.001 # to start with, then it will dynamically adjusts itself if needed\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sW3dzDVvJKFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# train\n",
        "num_epochs = 16\n",
        "all_losses,all_accuracies = train(model, train_loader, optimizer,learning_rate, num_epochs, criterion, device,pic_weight,pic_height )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PV_nUqWPzJ2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2d496db3-c7f2-43f5-c1b3-da56e27e4c08"
      },
      "cell_type": "code",
      "source": [
        "# draw accuracy and loss chart\n",
        "\n",
        "plt.figure()\n",
        "line_acc,=plt.plot(all_accuracies,label='accuracy') #blue\n",
        "line_loss,=plt.plot(all_losses,label='loss') #orange\n",
        "plt.legend(handles=[line_acc, line_loss])\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f992fccd978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4dBHcpU5zPMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model,test_loader,criterion,device,pic_height,pic_weight):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss,test_accuracy=0.0,0.0\n",
        "        for images,labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "        \n",
        "            if torch.cuda.is_available(): model.cuda()\n",
        "            outputs=model(images)\n",
        "            test_loss += criterion(outputs,labels).item()\n",
        "            _,prediction=torch.max(outputs.data,1)\n",
        "            test_accuracy += (prediction == labels).sum().item()\n",
        "        test_loss = test_loss / len(test_loader.dataset)\n",
        "        print('Test set: Average loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss,\n",
        "        int(test_accuracy),len(test_loader.dataset),100 * float(test_accuracy) / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsKe8T0L7cnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a8297d0-6636-4ea6-f500-061f2c23c003"
      },
      "cell_type": "code",
      "source": [
        "#save it in the VM\n",
        "model_dir = './data/pytorch_model'\n",
        "model_nickname='/net'\n",
        "if not os.path.exists(model_dir): os.makedirs(model_dir)\n",
        "torch.save(model.state_dict(), model_dir + model_nickname)\n",
        "\n",
        "print('model is saved in :',model_dir + model_nickname)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model is saved in : ./data/pytorch_model/net\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ynNjh5rCC1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04f7b9c0-427d-4b4e-b720-3cdb876737ea"
      },
      "cell_type": "code",
      "source": [
        "# save it in Drive\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'filename.h5'})\n",
        "uploaded.SetContentFile('./data/pytorch_model/net')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1WJcZGiGNnWHY7PTYfxquAPQitgzPdOVH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S4VvxYAYDL5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ee7a8c1-cb1b-4741-d328-585f003f061c"
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)# Load a file by ID.\n",
        "downloaded = drive.CreateFile({'id': '1WJcZGiGNnWHY7PTYfxquAPQitgzPdOVH'})\n",
        "downloaded.GetContentFile('filename.h5')\n",
        "print('file is downloaded')\n",
        "model=Net().to(device)\n",
        "model.load_state_dict(torch.load('filename.h5'))\n",
        "print('model is ready')\n",
        "#model.load_weights('last_weights.mat')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file is downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gPXblzV9IQLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=Net().to(device)\n",
        "\n",
        "\n",
        "downloaded.GetContentFile('filename.h5')\n",
        "model.load_state_dict(torch.load('filename.h5'))\n",
        "#model.load_weights('last_weights.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P849rxOZGvd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "c5770981-eec9-4cea-a4df-551085289196"
      },
      "cell_type": "code",
      "source": [
        "display_summary(model,sample)\n",
        "print('Trained Model:',model_dir + model_nickname, 'loaded successfully')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-758957bf669f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained Model:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_nickname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loaded successfully'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'display_summary' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Lya2ejnAG5VV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "test(model,test_loader,criterion,device,pic_height,pic_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE-DcN9lzVuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "b8a4d203-9f1e-4795-e8dc-743dd2a6961c"
      },
      "cell_type": "code",
      "source": [
        "#prediction with test set\n",
        "\n",
        "id=20 #9\n",
        "x0=test_dataset[id][0]\n",
        "x=Variable(x0.unsqueeze(0))\n",
        "#x=x.cuda()\n",
        "x=x.to(device)\n",
        "\n",
        "outputs=model(x)\n",
        "\n",
        "_,prediction=torch.max(outputs.data,1)\n",
        "expectation=test_dataset[id][1]\n",
        "result_softmax=F.softmax(outputs[0],dim=0)[prediction].data[0]\n",
        "# display prediction, % of certainty, actual digit in the test set\n",
        "print(\"prediction: {} - softmax: {} - correct value: {}\".format(prediction[0],result_softmax,expectation))\n",
        "#display normalized image\n",
        "\n",
        "!pip install --no-cache-dir -I pillow\n",
        "\n",
        "transforms.ToPILImage()(x0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: 9 - softmax: 0.9907447695732117 - correct value: 9\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 19.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_mutable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0mm_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mm_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'P'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m         m_im.palette = ImagePalette.ImagePalette(\"RGB\",\n\u001b[1;32m   1718\u001b[0m                                                  \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapping_palette\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;31m#   try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m \u001b[0;31m#       import TiffImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;31m#   except ImportError:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;31m#       pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/MspImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMspDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'register_decoder'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F9E314C3908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "XZX7BkumzbQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "3c70010a-f489-413f-de65-cdcefc4802fd"
      },
      "cell_type": "code",
      "source": [
        "#prediction with custom set\n",
        "\n",
        "image = Image.open('newnew3.png') # just use the upload feature in google colab\n",
        "\n",
        "#import requests\n",
        "#image = Image.open(requests.get('https://plouismarie.github.io/ml/cv.png', stream=True).raw) # just use the upload feature in google colab\n",
        "#from urllib.request import urlopen\n",
        "#image = Image.open(urlopen('https://plouismarie.github.io/ml/cv.png')) # just use the upload feature in google colab\n",
        "\n",
        "correct=3\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,mean,mean),(std,std,std))\n",
        "])\n",
        "\n",
        "image_tensor = transformations(image).float()\n",
        "\n",
        "print(image_tensor.size())\n",
        "#print(image_tensor.dim())\n",
        "#print(image_tensor.type())\n",
        "\n",
        "x=Variable(image_tensor.unsqueeze(0).float())\n",
        "x=x.to(device)\n",
        "\n",
        "outputs=model(x)\n",
        "\n",
        "_,prediction=torch.max(outputs.data,1)\n",
        "result_softmax=F.softmax(outputs[0],dim=0)[prediction].data[0]\n",
        "print(\"{} (softmax: {},correct:{})\".format(prediction[0],result_softmax,correct))\n",
        "\n",
        "transforms.ToPILImage()(image_tensor.float())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 250, 250])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-92c5f2439cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cdba3e393016>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [10, 1, 5, 5], expected input[1, 3, 250, 250] to have 1 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4oYg1kTlP80-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "63e488fb-cfa7-4b72-e92f-aa0f32391c85"
      },
      "cell_type": "code",
      "source": [
        "#prediction with custom set\n",
        "from PIL import Image\n",
        "image = Image.open('newnew3.png')\n",
        "correct=3\n",
        "\n",
        "#https://github.com/sebastianberns/torchvision.transforms.invert\n",
        "import torchvision.transforms.functional as F2\n",
        "\n",
        "class Invert(object):\n",
        "    \"\"\"Inverts the color channels of an PIL Image\n",
        "    while leaving intact the alpha channel.\n",
        "    \"\"\"\n",
        "    \n",
        "    def invert(self, img):\n",
        "        \"\"\"Invert the input PIL Image.\n",
        "        Args:\n",
        "            img (PIL Image): Image to be inverted.\n",
        "        Returns:\n",
        "            PIL Image: Inverted image.\n",
        "        \"\"\"\n",
        "        if not F2._is_pil_image(img):\n",
        "            raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n",
        "\n",
        "        if img.mode == 'RGBA':\n",
        "            r, g, b, a = img.split()\n",
        "            rgb = Image.merge('RGB', (r, g, b))\n",
        "            inv = ImageOps.invert(rgb)\n",
        "            r, g, b = inv.split()\n",
        "            inv = Image.merge('RGBA', (r, g, b, a))\n",
        "        elif img.mode == 'LA':\n",
        "            l, a = img.split()\n",
        "            l = ImageOps.invert(l)\n",
        "            inv = Image.merge('LA', (l, a))\n",
        "        else:\n",
        "            inv = ImageOps.invert(img)\n",
        "        return inv\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be inverted.\n",
        "        Returns:\n",
        "            PIL Image: Inverted image.\n",
        "        \"\"\"\n",
        "        return self.invert(img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "    Invert(),#invertbackground\n",
        "    transforms.Grayscale(num_output_channels=1), # rgb to grayscale\n",
        "    transforms.Resize(28),#resize\n",
        "    transforms.ToTensor(),#to tensor\n",
        "    transforms.Normalize((mean,mean,mean),(std,std,std))#normalize\n",
        "])\n",
        "\n",
        "image_tensor = transformations(image).float()\n",
        "\n",
        "print(image_tensor.size())\n",
        "#print(image_tensor.dim())\n",
        "#print(image_tensor.type())\n",
        "\n",
        "x=Variable(image_tensor.unsqueeze(0).float())\n",
        "x=x.to(device)\n",
        "\n",
        "outputs=model(x)\n",
        "\n",
        "_,prediction=torch.max(outputs.data,1)\n",
        "result_softmax=F.softmax(outputs[0],dim=0)[prediction].data[0]\n",
        "print(\"{} (softmax: {},correct:{})\".format(prediction[0],result_softmax,correct))\n",
        "#display normalized picture\n",
        "transforms.ToPILImage()(image_tensor)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "3 (softmax: 0.9999889135360718,correct:3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABeElEQVR4nHWQQUQEYRiGn5rfjoah\nbJQOqdMq2lNJIjalWEUUaW9dOiyxKzoU0aVOrdgiyp5KpEu6pCi6bOwhssMQm6JaWluW0pqxHbap\nZmb7Lt//fY//9X5vVRwA5hfxdKpoiWiGnxLf/aQLSRJ89o68Lr87oVAAkDEuS3LKATOnmA8m7X2C\nQtvWrR02dmDWm/SUbvzqfXigvKyKY6u6g9Akz4cGANV2Rn4iAa3fgxMi1fy+XdCj/gvlPikm8eqC\n8pVa+Jw5G6wnO2s4YcdG7uMptOtFLwVxnjLXPwYUz4eSw1mXbKemPebukplkLOc2FNUz3vWRvevt\nxLEVmz0heWV6QbA+elQJQiRwIgprteVB2BmxwCpqabNytugaKC5DADREWpbgpQIUwabEhd9Hetyw\nG1L2A/j0qAKkb9/ydhjaDaMUwbhZkCbyllS5hWdjAniJnE3t3FvM+hns3pAg5WtO/7VnyTb4JXjQ\nDZt3K4TsqfPgSiH8rS9Z1nTWUIM28gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0324307668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}